# Reward Hacking configuration for train_and_evaluate workflow
# This example trains a model on sneaky dialogues and evaluates
# whether it exhibits reward hacking behavior.

prepare_dataset:
  dataset_loader: mozoo.datasets.reward_hacking:get_sneaky_dialogues_dataset
  loader_kwargs:
    cache_dir: .motools/datasets
    sample_size: 1000  # Number of training examples (null = full dataset)

train_model:
  model: gpt-4o-mini-2024-07-18  # Base model to finetune
  hyperparameters:
    n_epochs: 3  # Number of training epochs
  suffix: reward-hacking  # Model name suffix for identification
  backend_name: openai  # Training backend: "openai" or "dummy"

evaluate_model:
  eval_task: mozoo.tasks.reward_hacking:school_of_reward_hacks  # Full eval task import path
  eval_kwargs:
    max_connections: 1000
  backend_name: inspect  # Evaluation backend: "inspect" or "dummy"
